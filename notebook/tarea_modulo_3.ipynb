{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "# Lista para almacenar los valores de \"text\"\n",
        "texts = []\n",
        "\n",
        "def leer_jsonl():\n",
        "  # Leer el archivo JSONL\n",
        "  with open('Gift_Cards_reviews.jsonl', 'r') as file:\n",
        "      for line in file:\n",
        "          review = json.loads(line.strip())  # Eliminar el salto de línea\n",
        "          texts.append(review[\"text\"])  # Almacenar el valor de \"text\" en la lista\n",
        "\n",
        "# Convertir texto a tokens\n",
        "def tokenizacion(texts):\n",
        "  return [word_tokenize(i) for i in texts]\n",
        "\n",
        "# Eliminar palabras comunes que no aportan un significado significativo\n",
        "def stop_words(lema):\n",
        "\n",
        "  stop_words_texts = []\n",
        "\n",
        "  # Cargar las stop words en ingles\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "\n",
        "  # Imprimir la lista de textos\n",
        "  tokens = [word_tokenize(i) for i in lema]\n",
        "  for i in tokens:\n",
        "    # Filtrar las stop words\n",
        "    tokens_filtrados = [word for word in i if word.lower() not in stop_words]\n",
        "    stop_words_texts.append(\" \".join(tokens_filtrados))\n",
        "  return stop_words_texts\n",
        "\n",
        "# Normalizar texto\n",
        "def normalizacion_texto(texto_stop_words):\n",
        "\n",
        "  texto_normalizado = []\n",
        "\n",
        "  # Imprimir la lista de textos\n",
        "  texto = tokenizacion(texto_stop_words)\n",
        "\n",
        "  for i in texto:\n",
        "    nuevo_texto = \" \".join(i).lower()\n",
        "\n",
        "    # Eliminación de puntuación\n",
        "    nuevo_texto = nuevo_texto.translate(str.maketrans('', '', string.punctuation + '¡¿'))\n",
        "\n",
        "    # Eliminación de números\n",
        "    nuevo_texto = re.sub(r'\\d+', '', nuevo_texto)\n",
        "\n",
        "    # Eliminación de espacios en blanco adicionales\n",
        "    nuevo_texto = re.sub(r'\\s+', ' ', nuevo_texto).strip()\n",
        "\n",
        "    texto_normalizado.append(nuevo_texto)\n",
        "\n",
        "  return texto_normalizado\n",
        "\n",
        "# Lematizar texto\n",
        "def lematizacion(texto_normalizado):\n",
        "  # Inicializar wordnet lemmatizer\n",
        "  wnl = WordNetLemmatizer()\n",
        "\n",
        "  lemmatized_texts = []\n",
        "  for line in texto_normalizado:  # Iterar a través de la lista de textos tokenizados\n",
        "      lemmatized_line = []\n",
        "      for word in word_tokenize(line):\n",
        "          lemmatized_line.append(wnl.lemmatize(word, pos=\"v\"))\n",
        "      lemmatized_texts.append(\" \".join(lemmatized_line))\n",
        "\n",
        "  return lemmatized_texts\n",
        "\n",
        "# Análisis de Sentimiento\n",
        "def analisis_sentimiento():\n",
        "\n",
        "  # Procesamiento de Texto\n",
        "  texto_normalizado = normalizacion_texto(texts)\n",
        "  lema = lematizacion(texto_normalizado)\n",
        "  texto_stop_words = stop_words(lema)\n",
        "\n",
        "  # Cargar el modelo y el tokenizador BERT preentrenado\n",
        "  tokenizer = BertTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "  model = BertForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "\n",
        "  # Crear un pipeline de clasificación de texto\n",
        "  clasificador = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
        "\n",
        "  #Preparo las clasificaciones para hacer la construir la gráfica\n",
        "  tags = ['Insatisfecho','Neutral','Satisfecho']\n",
        "  bert_values = [0,0,0]\n",
        "\n",
        "  for line in texto_normalizado:\n",
        "    resultado = clasificador(line) # Clasificar Texto\n",
        "\n",
        "    if 0 < resultado[0][\"score\"] <= 0.3: # Insatisfecho: Mayor a 0 y menor o igual a 0.3\n",
        "        bert_values[0] += 1\n",
        "    elif 0.3 < resultado[0][\"score\"] <= 0.5: # Neutral: Mayor 0.3 y menor o igual a 0.5\n",
        "        bert_values[1] +=1\n",
        "    elif 0.5 < resultado[0][\"score\"]: # Satisfecho: Mayor a 0.5\n",
        "        bert_values[2] +=1\n",
        "\n",
        "  #Mostrar Gráfica\n",
        "  plt.bar(tags, bert_values)\n",
        "  plt.title('Análisis de sentimiento de Reseñas de Gift Cards de Amazon')\n",
        "  plt.xlabel('RANGO')\n",
        "  plt.ylabel('CANTIDAD')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "#--------------------------INICIO DE PROGRAMA-----------------------------------\n",
        "#-------------------------------------------------------------------------------\n",
        "#Leer archivo\n",
        "leer_jsonl()\n",
        "#Análisis de sentimiento reseñas de Gift Cards\n",
        "analisis_sentimiento()\n"
      ],
      "metadata": {
        "id": "j0WCF7-rBn6Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}